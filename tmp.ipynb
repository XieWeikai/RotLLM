{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "path = \"/data/share/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "device = \"cuda:7\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path, device_map=device, torch_dtype=dtype)\n",
    "model = model.eval()\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What can you do for me?\"},\n",
    "]\n",
    "\n",
    "prompt = tok.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n",
    "inp = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "print(tok.batch_decode(output, skip_special_tokens=False)[0])\n",
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qwen_utils\n",
    "import rotation_utils\n",
    "\n",
    "dim = model.config.hidden_size\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = dim // num_heads\n",
    "R = rotation_utils.get_orthogonal_matrix(dim, mode=\"hadamard\", device=device)\n",
    "R_v = rotation_utils.get_orthogonal_matrix(head_dim, mode=\"hadamard\", device=device)\n",
    "\n",
    "qwen_utils.untie_word_embeddings(model)\n",
    "qwen_utils.fuse_layer_norms(model)\n",
    "qwen_utils.rotate_model(model, R, R_v)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "    )\n",
    "print(tok.batch_decode(output, skip_special_tokens=False)[0])\n",
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af6d22f7eac4ba6bbb9f65c0a06a5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image is an illustration featuring a group of characters from the anime \"Spy x Family.\" The characters are depicted in a stylized, cartoonish manner, with a focus on their distinctive outfits and poses. The characters are arranged in a grid format, with each character occupying a different section of the image. The background is a light teal color, and the text \"SPYÃ—FAMILY\" is prominently displayed at the top in bold, black letters. The overall style is reminiscent of the anime\\'s art style, which is known for its detailed and expressive character designs.']\n",
      "Qwen2VLForConditionalGeneration(\n",
      "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2VLVisionBlock(\n",
      "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): VisionFlashAttention2(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): VisionMlp(\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (act): QuickGELUActivation()\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): PatchMerger(\n",
      "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2VLModel(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
      "        (self_attn): Qwen2VLFlashAttention2(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2ForCausalLM\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_path = \"/data/share/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda:7\"\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     model_path, torch_dtype=dtype, device_map=device\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "# default processer\n",
    "# processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(model_path, min_pixels=min_pixels, max_pixels=max_pixels, use_fast=False)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"./aniya.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tie word embeddings, clone lm_head from embed_tokens\n",
      "['The image is an illustration featuring a group of characters from the anime \"Spy x Family.\" The characters are depicted in a stylized, cartoonish manner, with a focus on their distinctive outfits and poses. The characters are arranged in a grid-like formation, with each character positioned in a different pose and attire. The background is a solid light blue color, and the characters are set against a darker blue circle, creating a striking contrast. The text \"SPYÃ—FAMILY\" is prominently displayed at the top of the image in bold, black letters. The overall style is reminiscent of the anime\\'s art style, which is known for its']\n"
     ]
    }
   ],
   "source": [
    "import qwen_utils\n",
    "import rotation_utils\n",
    "\n",
    "qwen_utils.untie_word_embeddings(model)\n",
    "qwen_utils.fuse_layer_norms(model)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image is an illustration featuring six characters from the anime \"Spy x Family.\" The characters are arranged in a grid format, with each character positioned in a different quadrant of the grid. The characters are depicted in various poses and expressions, creating a dynamic and engaging visual. The background is a light teal color, and the text \"SPYÃ—FAMILY\" is prominently displayed at the top of the image in bold, black letters. The overall style is reminiscent of anime art, with detailed line work and vibrant colors.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0479, -0.0479, -0.0601,  ...,  0.0820, -0.0250,  0.1182],\n",
       "        [ 0.0374,  0.0562, -0.2021,  ...,  0.0522, -0.0527,  0.0236],\n",
       "        [ 0.0339, -0.2217, -0.0058,  ..., -0.1055, -0.1245, -0.0518],\n",
       "        ...,\n",
       "        [-0.0112,  0.0099,  0.1602,  ..., -0.0269, -0.0164, -0.0635],\n",
       "        [-0.0112,  0.0099,  0.1602,  ..., -0.0269, -0.0165, -0.0635],\n",
       "        [-0.0112,  0.0099,  0.1602,  ..., -0.0269, -0.0165, -0.0635]],\n",
       "       device='cuda:7', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = model.config.hidden_size\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = dim // num_heads\n",
    "R = rotation_utils.get_orthogonal_matrix(dim, mode=\"hadamard\", device=device)\n",
    "R_v = rotation_utils.get_orthogonal_matrix(head_dim, mode=\"hadamard\", device=device)\n",
    "\n",
    "qwen_utils.rotate_model(model, R, R_v)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/data/share/Qwen2-VL-2B-Instruct-rotated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
